# Second Brain (Local-First AI Knowledge Base)

**Purpose**  
Personal AI-powered knowledge base. Start local, keep the stack minimal, deploy later.

## ‚úÖ Completed Features

### Day 1 - Foundation
- ‚úÖ Repo skeleton with Next.js App Router
- ‚úÖ Environment files configured
- ‚úÖ SQLite via Prisma
- ‚úÖ Health check route at `/api/health`

### Day 2 - PDF Upload & Storage
- ‚úÖ PDF upload functionality (single and multiple files)
- ‚úÖ File validation (PDF only, max 20MB)
- ‚úÖ Sanitized filename storage
- ‚úÖ Document persistence in SQLite
- ‚úÖ Document library with upload UI

### Day 3 - Text Extraction & Embeddings (FREE Local Ollama)
- ‚úÖ PDF text extraction using `pdf-parse`
- ‚úÖ Text chunking with overlap (1500 chars, 200 overlap)
- ‚úÖ **FREE local embeddings** using Ollama (no API costs!)
- ‚úÖ Chunk and Embedding storage in database
- ‚úÖ Auto-embedding on upload (no manual "Embed" button needed)

### Day 4+ - Conversation-Scoped RAG & Data Management
- ‚úÖ **Conversation model**: Each chat has its own context
- ‚úÖ **Scoped uploads**: Documents belong to specific conversations
- ‚úÖ **Scoped queries**: Answers only use files from the current chat
- ‚úÖ **Cascading deletes**: Remove a chat ‚Üí removes all its files, chunks, and embeddings
- ‚úÖ **Sidebar with chat list**: Create, select, and delete conversations
- ‚úÖ **Multi-file upload**: Up to 10 PDFs at once with auto-embedding

### Streaming Responses
- ‚úÖ **Token-by-token streaming**: Responses appear as they're generated by Ollama
- ‚úÖ **Stop generation**: Click "Stop generating" button to cancel mid-stream
- ‚úÖ **Real-time feedback**: See answers being built in real-time
- ‚úÖ **Graceful cancellation**: Partial answers are preserved when stopped

## Local Setup

### Prerequisites
- Node.js LTS (v18 or higher)
- **Ollama** (for FREE local embeddings)

### Installation

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Install Ollama (FREE local AI):**
   ```bash
   # Install Ollama
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Start Ollama server (in a new terminal)
   ollama serve
   
   # Pull the embedding model (in another terminal)
   ollama pull all-minilm
   ```

3. **Configure environment:**
   ```bash
   cp .env.example .env.local
   ```
   
   The default `.env.local` should have:
   ```
   OLLAMA_BASE=http://localhost:11434
   OLLAMA_EMBED_MODEL=all-minilm
   DATABASE_URL=file:/home/shubon/Desktop/Second-Brain/prisma/data/app.db
   UPLOAD_DIR=./uploads
   ```

4. **Run migrations:**
   ```bash
   npx prisma migrate dev
   ```

5. **Start development server:**
   ```bash
   npm run dev
   ```

6. **Open in browser:**
   ```
   http://localhost:3000
   ```

## Structure
- `/app` ‚Äî Next.js App Router area
- `/app/api` ‚Äî server routes (same project, local-first)
- `/data` ‚Äî SQLite database file (git-ignored)
- `/uploads` ‚Äî local file uploads (git-ignored)
- `/prisma` ‚Äî Prisma schema & migrations

## Environment
- Copy `.env.example` to `.env.local` and fill in values locally.
- Never commit secrets.

## Usage

### Creating and Managing Conversations
1. Click "**+ New Chat**" in the left sidebar to create a new conversation
2. Each conversation maintains its own:
   - Chat history (messages and answers)
   - Uploaded documents and their embeddings
   - Context for queries (answers only use files from the current chat)
3. Click on a conversation in the sidebar to switch to it
4. Click the üóëÔ∏è (trash) icon to delete a conversation
   - **Warning**: Deleting removes all messages, uploaded files, and embeddings for that chat

### Uploading Documents
1. **First**, create or select a conversation in the sidebar
2. Click the "**+**" (Add files) button in the chat interface
3. Select one or more PDF files (up to 10 files at once, max 20MB each)
4. Or drag-and-drop up to 10 PDF files into the chat area
5. Files are uploaded sequentially and **automatically embedded**:
   - Files saved to `./uploads/`
   - Text extracted and chunked
   - Embeddings generated via Ollama
   - Success toast shows: `Uploaded filename.pdf ‚Äî embedded ‚úì (X chunks)`
6. Documents are scoped to the current conversation

### Asking Questions
1. Ensure you have a conversation selected with uploaded documents
2. Type your question in the input box and press Enter (or click Send)
3. The system will:
   - Search only within the current conversation's documents
   - Find the most relevant chunks using similarity search
   - Generate an answer using your local LLM
4. If no documents are in the conversation, you'll see: "No embedded content in this conversation yet"

### Viewing Data in Prisma Studio
```bash
npx prisma studio
```
This opens a GUI at `http://localhost:5555` where you can view:
- **Document** table - Uploaded PDFs
- **Chunk** table - Text chunks from PDFs
- **Embedding** table - Vector embeddings for each chunk

## Day 3 Testing - FREE Local Embeddings with Ollama

### Prerequisites Check:

**1. Verify Ollama is installed:**
```bash
ollama --version
```

**2. Start Ollama server (if not already running):**
```bash
# In a separate terminal
ollama serve
```

**3. Pull the embedding model:**
```bash
ollama pull all-minilm
```

**4. Test Ollama directly (optional):**
```bash
curl -s http://localhost:11434/api/embed \
  -H 'Content-Type: application/json' \
  -d '{"model":"all-minilm","input":"Hello world"}'
```

Expected: JSON response with `{"embeddings":[[...]]}`

### Test the Full Embedding Pipeline:

1. **Start your dev server:**
   ```bash
   npm run dev
   ```

2. **Upload a PDF** (if you haven't already):
   - Go to `http://localhost:3000`
   - Click "+ Upload PDF"
   - Select a PDF file

3. **Trigger embedding:**
   - Find your document in the table
   - Click "**Embed**" button next to it
   - Expected UI: 
     ```
     Embedding... ‚Üí ‚úì Embedded
     18 chunks, 18 embeddings
     ```
   (Numbers vary by document size)

4. **Check server logs:**
   Server console should show:
   ```
   [Embed] Starting embedding process for document X
   [Embed] Found document: filename.pdf
   [Embed] Reading PDF from: /path/to/file
   [Embed] Extracted XXXX characters of text
   [Embed] Created XX chunks
   [Embed] Inserting XX chunks into database...
   [Ollama] Embedding XX texts using model: all-minilm
   [Ollama] Processing batch 1 (XX texts)
   [Ollama] Successfully generated XX embeddings
   [Embed] Generated XX embeddings
   [Embed] ‚úì Embedding process complete for document X
   ```

5. **Verify in Prisma Studio:**
   ```bash
   npx prisma studio
   ```
   - Navigate to **Chunk** table - should see text chunks
   - Navigate to **Embedding** table - should see vector embeddings
   - Check `model` column shows "all-minilm"

### Common Issues:

**Error: "Cannot connect to Ollama server"**
```bash
# Solution: Start Ollama
ollama serve
```

**Error: "Model 'all-minilm' not found"**
```bash
# Solution: Pull the model
ollama pull all-minilm
```

**Want to use a different model?**
```bash
# Pull a different model
ollama pull mxbai-embed-large

# Update .env.local
OLLAMA_EMBED_MODEL=mxbai-embed-large

# Restart your dev server
```

## Day 3 - Polish & Error Handling

### Duplicate Prevention
Embedding the same document twice will return a **409 Conflict** error:
```json
{
  "error": "Document already embedded. To refresh later, implement a re-embed mode."
}
```

This prevents duplicate chunks and embeddings from being created accidentally.

### Error Handling & Troubleshooting

The embed endpoint now provides clearer error messages:

**400 Bad Request**
- Missing or invalid `documentId` in request body

**404 Not Found**
- Document doesn't exist
- PDF file path missing or file not found
- Ollama model not found ‚Üí **Fix:** `ollama pull <model>`

**409 Conflict**
- Document already embedded ‚Üí **Fix:** Use a different document or implement re-embed mode

**422 Unprocessable Entity**
- No extractable text (scanned PDF) ‚Üí **Fix:** Use OCR or a text-based PDF

**503 Service Unavailable**
- Ollama server not running ‚Üí **Fix:** Start with `ollama serve`

**Example Error Response:**
```json
{
  "error": "Cannot connect to Ollama server. Start: `ollama serve`"
}
```

### UI Error Display
The embed button now shows:
- **"Embedding..."** while processing
- **"Done ‚úì (X chunks)"** on success
- **Error message** from server on failure (plain text, easy to read)

## API Endpoints

### GET `/api/conversations`
List all conversations.

**Response:**
```json
{
  "conversations": [
    {
      "id": 1,
      "title": "New chat",
      "createdAt": "2025-11-07T...",
      "updatedAt": "2025-11-07T..."
    }
  ]
}
```

### POST `/api/conversations`
Create a new conversation.

**Request:**
```json
{
  "title": "My Research Chat"
}
```

**Response (201):**
```json
{
  "id": 2,
  "title": "My Research Chat",
  "createdAt": "2025-11-07T...",
  "updatedAt": "2025-11-07T..."
}
```

### GET `/api/conversations/[id]`
Get conversation with messages.

**Response:**
```json
{
  "id": 1,
  "title": "New chat",
  "messages": [
    {
      "id": 1,
      "role": "user",
      "content": "What is this about?",
      "createdAt": "2025-11-07T...",
      "sources": []
    }
  ]
}
```

### DELETE `/api/conversations/[id]`
Delete conversation with cascading deletes (messages, documents, chunks, embeddings, files).

**Response:**
```json
{
  "deleted": true,
  "filesRemoved": 3,
  "docsRemoved": 3
}
```

### POST `/api/upload`
Upload a PDF file, attach to conversation, and auto-embed.

**Request:** `multipart/form-data` with:
- `file` - PDF file
- `conversationId` - Conversation ID (required)

**Response (201):**
```json
{
  "documentId": 1,
  "filename": "1234567890-document.pdf",
  "sizeBytes": 453261,
  "conversationId": 1,
  "autoEmbedded": true,
  "chunksCreated": 18,
  "embeddingsCreated": 18,
  "model": "all-minilm"
}
```

**Error Responses:**
- **400**: Missing file or invalid conversationId
- **404**: Conversation not found, or Ollama model not found
- **422**: No extractable text (scanned PDF)
- **503**: Ollama server not running

### POST `/api/query`
Ask a question scoped to a conversation.

**Request:**
```json
{
  "question": "What is the main idea?",
  "conversationId": 1
}
```

**Response:**
```json
{
  "answer": "Based on the documents...",
  "sources": [
    {
      "documentId": 1,
      "filename": "document.pdf",
      "chunkIndex": 0
    }
  ],
  "used": {
    "k": 5,
    "model": "llama3"
  }
}
```

**Error Responses:**
- **400**: Missing question or conversationId
- **404**: Conversation not found
- **422**: No embedded content in this conversation
- **503**: Ollama server not running

### POST `/api/query-stream`
Ask a question with streaming response (token-by-token).

**Request:**
```json
{
  "question": "What is the main idea?",
  "conversationId": 1
}
```

**Response:** NDJSON stream (newline-delimited JSON)

Each line is a JSON object with one of these types:

1. **Sources** (sent first):
```json
{
  "type": "sources",
  "sources": [{"documentId": 1, "chunkId": 5, "filename": "doc.pdf", "chunkIndex": 0}],
  "model": "llama3"
}
```

2. **Delta** (streamed tokens):
```json
{"type": "delta", "delta": "Based "}
{"type": "delta", "delta": "on "}
{"type": "delta", "delta": "the "}
```

3. **Done** (completion signal):
```json
{
  "type": "done",
  "fullAnswer": "Based on the documents..."
}
```

4. **Error** (if something goes wrong):
```json
{
  "type": "error",
  "error": "Error message"
}
```

**Features:**
- Real-time token streaming
- Abortable with AbortController/signal
- Messages persisted to database after completion
- Graceful handling of cancellation

**Error Responses:** Same as `/api/query`

### GET `/api/health`
Health check endpoint.

**Response:**
```json
{
  "ok": true,
  "time": "2025-10-31T01:22:17.183Z"
}
```

## Database Schema

### Conversation
- `id` - Primary key
- `title` - Conversation title (e.g., "New chat")
- `createdAt` - Creation timestamp
- `updatedAt` - Last update timestamp
- `messages` - Related messages
- `documents` - Related documents

### Message
- `id` - Primary key
- `conversationId` - Foreign key to Conversation (cascades on delete)
- `role` - Message role ('user' or 'assistant')
- `content` - Message text
- `createdAt` - Timestamp
- `sources` - Related message sources

### MessageSource
- `id` - Primary key
- `messageId` - Foreign key to Message (cascades on delete)
- `documentId` - Document ID
- `chunkId` - Chunk ID
- `filename` - Document filename
- `chunkIndex` - Chunk index

### Document
- `id` - Primary key
- `filename` - Sanitized filename
- `originalPath` - File path on disk
- `sizeBytes` - File size in bytes
- `conversationId` - Foreign key to Conversation (nullable for legacy docs)
- `createdAt` - Upload timestamp

### Chunk
- `id` - Primary key
- `documentId` - Foreign key to Document (cascades on delete)
- `chunkIndex` - Chunk sequence number (0, 1, 2...)
- `text` - Chunk text content
- `tokenCount` - Approximate token count (character length)
- `createdAt` - Creation timestamp

### Embedding
- `id` - Primary key
- `chunkId` - Foreign key to Chunk (cascades on delete)
- `vector` - Embedding vector (JSON array of numbers)
- `model` - Model used (e.g., "all-minilm" from Ollama)
- `createdAt` - Creation timestamp

## Tech Stack
- **Framework:** Next.js 16 (App Router)
- **Database:** SQLite via Prisma
- **PDF Processing:** pdf-parse
- **AI/Embeddings:** Ollama (FREE local embeddings - all-minilm)
- **Language:** TypeScript
- **Runtime:** Node.js

## Why Ollama?
- ‚úÖ **100% FREE** - No API costs, no quotas
- ‚úÖ **Privacy** - All data stays on your machine
- ‚úÖ **Fast** - Local processing, no network latency
- ‚úÖ **Offline** - Works without internet
- ‚úÖ **Flexible** - Easy to swap models

## Day 4 - RAG Q&A with Local LLM (FREE)

### Overview
Ask questions about your documents! The system uses:
- **Similarity Search**: Finds relevant chunks using cosine similarity
- **Local LLM**: Generates answers using Ollama (llama3, mistral, etc.)
- **Source Citations**: Shows which chunks were used

### Prerequisites

**1. Pull an LLM model:**
```bash
# Option 1: Llama 3 (recommended, ~4.7GB)
ollama pull llama3

# Option 2: Mistral (smaller, ~4.1GB)
ollama pull mistral

# Option 3: Any other Ollama model
ollama pull <model-name>
```

**2. Configure environment:**
Update `.env.local`:
```bash
OLLAMA_LLM_MODEL=llama3  # or your chosen model
TOP_K=5                  # Number of chunks to retrieve
MAX_CONTEXT_CHARS=3000   # Max context size for LLM
```

### Usage

**1. Start services:**
```bash
# Terminal 1: Ollama server
ollama serve

# Terminal 2: Your app
npm run dev
```

**2. Ask questions:**
- Go to `http://localhost:3000/ask`
- Or click "Ask Questions ‚Üí" on the home page
- Type your question and click "Ask"

**3. Example questions:**
- "What is the main idea in my notes?"
- "Summarize the key points from the documents"
- "What did I learn about blockchain?"

### How It Works

1. **Question Embedding**: Your question is converted to a vector
2. **Similarity Search**: Finds top-K most similar chunks (cosine similarity)
3. **Context Building**: Selected chunks are formatted into context
4. **LLM Generation**: Local Ollama LLM generates an answer using the context
5. **Response**: Returns answer + source citations

### Configuration Knobs

**`TOP_K`** (default: 5)
- Number of chunks to retrieve for context
- Higher = more context, but may include irrelevant info
- Lower = more focused, but may miss important details

**`MAX_CONTEXT_CHARS`** (default: 3000)
- Maximum characters in the context sent to LLM
- Prevents context overflow
- Adjust based on your LLM's context window

### Troubleshooting

**Error: "Ollama LLM model not found"**
```bash
# Solution: Pull the model
ollama pull llama3  # or your chosen model

# Verify it's available
ollama list
```

**Error: "Cannot connect to Ollama server"**
```bash
# Solution: Start Ollama
ollama serve
```

**Error: "No embedded content yet"**
- Embed at least one document first using the "Embed" button on the home page

**Slow responses?**
- LLM generation can take 10-30 seconds depending on model size
- Smaller models (mistral) are faster but less accurate
- Larger models (llama3) are slower but more accurate

### API Endpoint

**POST `/api/query`**
```json
{
  "question": "What is the main idea?"
}
```

**Response:**
```json
{
  "answer": "Based on the documents...",
  "sources": [
    {
      "documentId": 1,
      "filename": "document.pdf",
      "chunkIndex": 0
    }
  ],
  "used": {
    "k": 5,
    "model": "llama3"
  }
}
```

**Test with curl:**
```bash
curl -X POST http://localhost:3000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question":"What is the main idea in my notes?"}'
```

## Next Steps
- Day 5: Polish UI and optional deployment

